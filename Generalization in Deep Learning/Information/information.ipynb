{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch import reshape\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.hidden = nn.ModuleList()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_sizes[0],bias=False)\n",
    "        for k in range(len(hidden_sizes)-1):\n",
    "            self.hidden.append(nn.Linear(hidden_sizes[k], hidden_sizes[k+1],bias=False))\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size,bias=False)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=self.input_layer(x)\n",
    "        x=x.relu()\n",
    "        for layer in self.hidden:\n",
    "            x=layer(x)\n",
    "            x=x.tanh()\n",
    "        x=self.output_layer(x)\n",
    "        x=x.sigmoid()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(12,[12,10,7,5,4,3],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(l,length):\n",
    "    new_l=[]\n",
    "    if len(l[0])<length:\n",
    "        for e in l:\n",
    "            new_l.append(e+[0])\n",
    "            new_l.append(e+[1])\n",
    "        return f(new_l,length)\n",
    "    else:\n",
    "        return l\n",
    "    \n",
    "X=f([[0],[1]],12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "for x in X:\n",
    "    if sum(x)<6:\n",
    "        y.append(0)\n",
    "    else:\n",
    "        y.append(1)\n",
    "\n",
    "X=np.array(X)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(inputs, targets, batchSize):\n",
    "\tfor i in range(0, inputs.shape[0], batchSize):\n",
    "\t\tyield (inputs[i:i + batchSize], targets[i:i + batchSize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(model.parameters(), lr=0.01)\n",
    "lossFunc = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "\tmodel.train()\n",
    "\tfor (batchX, batchY) in next_batch(X, y, 64):\n",
    "\t\tpredictions = model(batchX)\n",
    "\t\tloss = lossFunc(predictions, batchY.long())\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\treturn loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.544\n",
      "Epoch: 2 Loss: 0.542\n",
      "Epoch: 3 Loss: 0.540\n",
      "Epoch: 4 Loss: 0.538\n",
      "Epoch: 5 Loss: 0.536\n",
      "Epoch: 6 Loss: 0.534\n",
      "Epoch: 7 Loss: 0.532\n",
      "Epoch: 8 Loss: 0.531\n",
      "Epoch: 9 Loss: 0.529\n",
      "Epoch: 10 Loss: 0.528\n",
      "Epoch: 11 Loss: 0.526\n",
      "Epoch: 12 Loss: 0.525\n",
      "Epoch: 13 Loss: 0.523\n",
      "Epoch: 14 Loss: 0.522\n",
      "Epoch: 15 Loss: 0.520\n",
      "Epoch: 16 Loss: 0.519\n",
      "Epoch: 17 Loss: 0.518\n",
      "Epoch: 18 Loss: 0.517\n",
      "Epoch: 19 Loss: 0.515\n",
      "Epoch: 20 Loss: 0.514\n",
      "Epoch: 21 Loss: 0.513\n",
      "Epoch: 22 Loss: 0.512\n",
      "Epoch: 23 Loss: 0.511\n",
      "Epoch: 24 Loss: 0.509\n",
      "Epoch: 25 Loss: 0.508\n",
      "Epoch: 26 Loss: 0.507\n",
      "Epoch: 27 Loss: 0.506\n",
      "Epoch: 28 Loss: 0.505\n",
      "Epoch: 29 Loss: 0.504\n",
      "Epoch: 30 Loss: 0.503\n",
      "Epoch: 31 Loss: 0.502\n",
      "Epoch: 32 Loss: 0.501\n",
      "Epoch: 33 Loss: 0.499\n",
      "Epoch: 34 Loss: 0.498\n",
      "Epoch: 35 Loss: 0.497\n",
      "Epoch: 36 Loss: 0.496\n",
      "Epoch: 37 Loss: 0.495\n",
      "Epoch: 38 Loss: 0.494\n",
      "Epoch: 39 Loss: 0.493\n",
      "Epoch: 40 Loss: 0.492\n",
      "Epoch: 41 Loss: 0.491\n",
      "Epoch: 42 Loss: 0.490\n",
      "Epoch: 43 Loss: 0.489\n",
      "Epoch: 44 Loss: 0.488\n",
      "Epoch: 45 Loss: 0.486\n",
      "Epoch: 46 Loss: 0.485\n",
      "Epoch: 47 Loss: 0.484\n",
      "Epoch: 48 Loss: 0.483\n",
      "Epoch: 49 Loss: 0.482\n",
      "Epoch: 50 Loss: 0.481\n",
      "Epoch: 51 Loss: 0.480\n",
      "Epoch: 52 Loss: 0.479\n",
      "Epoch: 53 Loss: 0.477\n",
      "Epoch: 54 Loss: 0.476\n",
      "Epoch: 55 Loss: 0.475\n",
      "Epoch: 56 Loss: 0.474\n",
      "Epoch: 57 Loss: 0.473\n",
      "Epoch: 58 Loss: 0.472\n",
      "Epoch: 59 Loss: 0.470\n",
      "Epoch: 60 Loss: 0.469\n",
      "Epoch: 61 Loss: 0.468\n",
      "Epoch: 62 Loss: 0.466\n",
      "Epoch: 63 Loss: 0.465\n",
      "Epoch: 64 Loss: 0.464\n",
      "Epoch: 65 Loss: 0.462\n",
      "Epoch: 66 Loss: 0.461\n",
      "Epoch: 67 Loss: 0.459\n",
      "Epoch: 68 Loss: 0.458\n",
      "Epoch: 69 Loss: 0.456\n",
      "Epoch: 70 Loss: 0.454\n",
      "Epoch: 71 Loss: 0.452\n",
      "Epoch: 72 Loss: 0.449\n",
      "Epoch: 73 Loss: 0.447\n",
      "Epoch: 74 Loss: 0.444\n",
      "Epoch: 75 Loss: 0.441\n",
      "Epoch: 76 Loss: 0.438\n",
      "Epoch: 77 Loss: 0.435\n",
      "Epoch: 78 Loss: 0.431\n",
      "Epoch: 79 Loss: 0.428\n",
      "Epoch: 80 Loss: 0.424\n",
      "Epoch: 81 Loss: 0.421\n",
      "Epoch: 82 Loss: 0.417\n",
      "Epoch: 83 Loss: 0.414\n",
      "Epoch: 84 Loss: 0.411\n",
      "Epoch: 85 Loss: 0.408\n",
      "Epoch: 86 Loss: 0.405\n",
      "Epoch: 87 Loss: 0.402\n",
      "Epoch: 88 Loss: 0.399\n",
      "Epoch: 89 Loss: 0.397\n",
      "Epoch: 90 Loss: 0.395\n",
      "Epoch: 91 Loss: 0.393\n",
      "Epoch: 92 Loss: 0.391\n",
      "Epoch: 93 Loss: 0.389\n",
      "Epoch: 94 Loss: 0.387\n",
      "Epoch: 95 Loss: 0.385\n",
      "Epoch: 96 Loss: 0.383\n",
      "Epoch: 97 Loss: 0.382\n",
      "Epoch: 98 Loss: 0.380\n",
      "Epoch: 99 Loss: 0.379\n",
      "Epoch: 100 Loss: 0.377\n"
     ]
    }
   ],
   "source": [
    "for n in range(100):\n",
    "    l=training()\n",
    "    print('Epoch: {} Loss: {:.3f}'.format(n+1,l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
