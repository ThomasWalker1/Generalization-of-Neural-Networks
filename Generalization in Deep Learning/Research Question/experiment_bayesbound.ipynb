{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 1\n",
    "HIDDEN_SIZE = 8\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_SIZE = 2\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.hidden = nn.ModuleList()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        for k in range(num_layers-1):\n",
    "            self.hidden.append(nn.Linear(hidden_size, hidden_size))\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=self.input_layer(x)\n",
    "        x=x.relu()\n",
    "        for layer in self.hidden:\n",
    "            x=layer(x)\n",
    "            x=x.relu()\n",
    "        x=self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "def ReLU_init(model):\n",
    "    for param in model.parameters():\n",
    "        nn.init.normal_(param, 0, 1/np.sqrt(HIDDEN_SIZE))\n",
    "\n",
    "def ReLU_glorot_init(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        \n",
    "        if name.endswith(\".bias\"):\n",
    "            param.data.fill_(0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 500\n",
    "NUM_TRAIN_SAMPLES = 50\n",
    "\n",
    "X=np.linspace(0,1,NUM_SAMPLES)\n",
    "\n",
    "breaks=np.sort(np.random.random(6))\n",
    "\n",
    "encode_inputs=lambda x: 1 if (x>breaks[0] and x<breaks[1]) or (x>breaks[2] and x<breaks[3]) or (x>breaks[4] and x<breaks[5]) else 0\n",
    "Y=np.array([encode_inputs(x) for x in X])\n",
    "\n",
    "X = torch.from_numpy(X).float()\n",
    "Y = torch.from_numpy(Y).float()\n",
    "\n",
    "TRAIN_INDICES = np.random.choice(np.arange(NUM_SAMPLES),size=NUM_TRAIN_SAMPLES,replace=False)\n",
    "X_TRAIN = X[TRAIN_INDICES]\n",
    "Y_TRAIN = Y[TRAIN_INDICES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(model):\n",
    "    weights=[]\n",
    "    for para in model.parameters():\n",
    "        weights+=torch.flatten(para).tolist()\n",
    "    return weights\n",
    "\n",
    "def next_batch(inputs, targets, batchSize):\n",
    "    for i in range(0, inputs.shape[0], batchSize):\n",
    "        yield (inputs[i:i + batchSize], targets[i:i + batchSize])\n",
    "\n",
    "def get_bound(feature_dataset, label_dataset, feature_train, label_train, deltas, lmda, print_loss=False):\n",
    "    model = NeuralNet(INPUT_SIZE,HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE)\n",
    "    ReLU_init(model)\n",
    "            \n",
    "    opt = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    lossFunc = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_samples=len(feature_dataset)\n",
    "    num_train=len(feature_train)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for x,y in next_batch(feature_train,label_train,BATCH_SIZE):\n",
    "            x=x.reshape((len(x),1))\n",
    "            outputs = model(x)\n",
    "            loss = lossFunc(outputs, y.long())\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if (epoch+1)%100==0 and print_loss:\n",
    "            print('Epoch {}: Loss {:.3f}'.format(epoch+1,loss.item()))\n",
    "\n",
    "    model_outputs=model(feature_dataset.reshape(len(feature_dataset),1)).max(1)[1]\n",
    "    classified=label_dataset==model_outputs\n",
    "\n",
    "    p_Delta=(sum(classified)/num_samples).numpy()\n",
    "\n",
    "    train_error=0\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in next_batch(feature_train,label_train,5):\n",
    "            output=model(x.reshape(len(x),1))\n",
    "            loss = lossFunc(output, y.long())\n",
    "            train_error+=loss.item()*len(x)\n",
    "        train_error=train_error/num_train\n",
    "\n",
    "    true_error=0\n",
    "    C=0\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in next_batch(feature_dataset,label_dataset,1):\n",
    "            output=model(x.reshape(len(x),1))\n",
    "            loss = lossFunc(output, y.long())\n",
    "            if C<loss.item():\n",
    "                C=loss.item()\n",
    "            true_error+=loss.item()*len(x)\n",
    "        true_error=true_error/num_samples\n",
    "\n",
    "    w=get_weight(model)\n",
    "    d=len(w)\n",
    "    pdf = exp(-np.linalg.norm(w)**2/(2*HIDDEN_SIZE))\n",
    "    pdf = pdf/(2*np.pi*HIDDEN_SIZE)**(d/2)\n",
    "\n",
    "    bounds=[]\n",
    "    for delta in deltas:\n",
    "        log_component = p_Delta**num_train\n",
    "        for k in range(1,num_train):\n",
    "            log_component+=factorial(num_train)*exp(lmda**2*C**2/(8*k))*p_Delta**(num_train-k)*(1-p_Delta)**k/(factorial(num_train-k)*factorial(k))\n",
    "        log_component=np.log(log_component)\n",
    "\n",
    "\n",
    "        bounds.append(train_error+(log_component+np.log(1/delta)+np.log(1/pdf))/lmda)\n",
    "    \n",
    "    return train_error, true_error, bounds, p_Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "92.51187314263723\n",
      "Trial 1 Training Error 0.306 Test Error 0.398\n",
      "106\n",
      "81.28836703855669\n",
      "Trial 2 Training Error 0.326 Test Error 0.425\n",
      "106\n",
      "140.21094853849243\n",
      "Trial 3 Training Error 0.169 Test Error 0.358\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thoma\\Dropbox\\ICL\\UROP 23\\GitHubRepo\\Generalization-of-Neural-Networks\\Generalization in Deep Learning\\Research Question\\experiment_bayesbound.ipynb Cell 6\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m emp_prob\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(DELTAS))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_TESTS):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     tr_er, ts_er, bs, p \u001b[39m=\u001b[39m get_bound(X,Y,X_TRAIN,Y_TRAIN,DELTAS, LAMBDA)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     train_errors\u001b[39m.\u001b[39mappend(tr_er)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     test_errors\u001b[39m.\u001b[39mappend(ts_er)\n",
      "\u001b[1;32mc:\\Users\\thoma\\Dropbox\\ICL\\UROP 23\\GitHubRepo\\Generalization-of-Neural-Networks\\Generalization in Deep Learning\\Research Question\\experiment_bayesbound.ipynb Cell 6\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m next_batch(feature_train,label_train,BATCH_SIZE):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     x\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mreshape((\u001b[39mlen\u001b[39m(x),\u001b[39m1\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     loss \u001b[39m=\u001b[39m lossFunc(outputs, y\u001b[39m.\u001b[39mlong())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     opt\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\thoma\\Dropbox\\ICL\\UROP 23\\GitHubRepo\\Generalization-of-Neural-Networks\\Generalization in Deep Learning\\Research Question\\experiment_bayesbound.ipynb Cell 6\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layer(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mrelu()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Dropbox/ICL/UROP%2023/GitHubRepo/Generalization-of-Neural-Networks/Generalization%20in%20Deep%20Learning/Research%20Question/experiment_bayesbound.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden:\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_TESTS=50\n",
    "N_DELTA=10\n",
    "DELTAS=[0.01,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.25]\n",
    "LAMBDA=1.1\n",
    "\n",
    "train_errors=[]\n",
    "test_errors=[]\n",
    "p_Deltas=[]\n",
    "trial_bounds=[[] for n in range(N_DELTA)]\n",
    "emp_prob=np.zeros(len(DELTAS))\n",
    "\n",
    "for k in range(NUM_TESTS):\n",
    "    tr_er, ts_er, bs, p = get_bound(X,Y,X_TRAIN,Y_TRAIN,DELTAS, LAMBDA)\n",
    "    train_errors.append(tr_er)\n",
    "    test_errors.append(ts_er)\n",
    "    p_Deltas.append(p)\n",
    "    for n, b in enumerate(bs):\n",
    "        trial_bounds[n].append(b)\n",
    "        if ts_er<b:\n",
    "            emp_prob[n]+=1\n",
    "    print('Trial {} Training Error {:.3f} Test Error {:.3f}'.format(k+1,tr_er,ts_er))\n",
    "emp_prob=emp_prob/NUM_TESTS\n",
    "print(emp_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
