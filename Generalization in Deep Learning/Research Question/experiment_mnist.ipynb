{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim import SGD\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 784\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 3\n",
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 50\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "NUM_SAMPLES = 2000\n",
    "NUM_TRAIN_SAMPLES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "dataset = torchvision.datasets.MNIST(root='../data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "sample_index=list(range(NUM_SAMPLES))\n",
    "train_index=list(np.random.choice(range(NUM_SAMPLES),size=NUM_TRAIN_SAMPLES,replace=False))\n",
    "\n",
    "sample_dataset=Subset(dataset, sample_index)\n",
    "train_dataset=Subset(sample_dataset, train_index)\n",
    "\n",
    "# Data loader\n",
    "TRAIN_LOADER = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "SAMPLE_LOADER = DataLoader(dataset=sample_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.hidden = nn.ModuleList()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        for k in range(num_layers-1):\n",
    "            self.hidden.append(nn.Linear(hidden_size, hidden_size))\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=self.input_layer(x)\n",
    "        x=x.relu()\n",
    "        for layer in self.hidden:\n",
    "            x=layer(x)\n",
    "            x=x.relu()\n",
    "        x=self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "def ReLU_glorot_init(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        \n",
    "        if name.endswith(\".bias\"):\n",
    "            param.data.fill_(0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bound(sample_loader, train_loader, num_samples, num_train, deltas):\n",
    "    model = NeuralNet(INPUT_SIZE,HIDDEN_SIZE, NUM_LAYERS, NUM_CLASSES)\n",
    "    ReLU_glorot_init(model)\n",
    "            \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for images, labels in train_loader:\n",
    "            images = images.reshape(-1, INPUT_SIZE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    train_error=0\n",
    "    with torch.no_grad():\n",
    "        for image, label in train_loader:\n",
    "            image = image.reshape(-1, INPUT_SIZE)\n",
    "            output = model(image)\n",
    "            loss = criterion(output, label)\n",
    "            train_error+=loss.item()*len(image)\n",
    "    train_error=train_error/num_train\n",
    "\n",
    "    true_error=0\n",
    "    classifications=[]\n",
    "    C=0\n",
    "    with torch.no_grad():\n",
    "        for image, label in sample_loader:\n",
    "            image = image.reshape(-1, INPUT_SIZE)\n",
    "            output = model(image)\n",
    "            loss = criterion(output, label)\n",
    "            if C<loss.item():\n",
    "                C=loss.item()\n",
    "            true_error+=loss.item()\n",
    "            predictions=output.max(1)[1]\n",
    "            classifications+=(predictions==label).tolist()\n",
    "    true_error=true_error/num_samples\n",
    "\n",
    "    bounds=[]\n",
    "    for delta in deltas:\n",
    "        log_component = np.log((1-p_Delta+np.sqrt((1-p_Delta)**2+4*delta**(1/num_train)*p_Delta))/(2*delta**(1/num_train)))\n",
    "\n",
    "        bounds.append(train_error+np.sqrt(C**2*log_component/2))\n",
    "    \n",
    "    return train_error, true_error, bounds, p_Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005451408843509853 0.4496843063832596 [0.8416651014148162, 0.7538554055048358, 0.6798802854636343, 0.6325769331416754, 0.5967247462587598, 0.5673428996717469, 0.5421437415333938, 0.5198762424884137, 0.4997773025382459, 0.46422944873351846] 0.888\n"
     ]
    }
   ],
   "source": [
    "DELTAS=[0.01,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.25]\n",
    "tr_er, ts_er, bs, p = get_bound(SAMPLE_LOADER, TRAIN_LOADER, NUM_SAMPLES, NUM_TRAIN, DELTAS)\n",
    "print(tr_er,ts_er, bs, p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
