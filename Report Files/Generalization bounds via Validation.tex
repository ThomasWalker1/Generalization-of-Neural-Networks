\documentclass[wide]{adonis}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remarks}[theorem]{Remarks}
\newtheorem{implementation}[theorem]{Implementation}

\title{Generalization Bounds via Validation}
\author{Thomas Walker}

\affiliation{Imperial College London}
\correspondence{thomas.walker21@imperial.ac.uk}
\date{July 2023}

\runningauthor{Thomas Walker}
\runningtitle{Generalization Bounds via Validation}

\begin{document}

\maketitle
\section{Notation}
\begin{itemize}
    \item $x\in\mathcal{X}$, input.
    \item $y\in\mathcal{Y}$, target.
    \item $\mathcal{L}$, loss function.
    \item $\mathcal{R}[f]=\mathbb{E}_{x,y\sim\mathbb{P}_{\mathcal{(X,Y)}}}\left(\mathcal{L}(f(x),y)\right)$, expected risk of a function $f$. Where $\mathbb{P}_{\mathcal{(X,Y)}}$ is the true distribution.
    \item $f_{\mathcal{A}(S)}:\mathcal{X}\to\mathcal{Y}$, function learnt by a learning algorithm $\mathcal{A}$ on training set $S:=S_m:=\{(x_1,y_1),\dots,(x_m,y_m)\}$
    \item $\mathcal{F}$, a characterized set of functions known as the hypothesis space.
    \item $\mathcal{L_F}:=\left\{g:g\in\mathcal{F},g(x,y)=\mathcal{L}(f(x),y)\right\}$, the family of loss functions associated to $\mathcal{F}$.
    \item For vector $v$ its dimension is $d_v$.
\end{itemize}
\section{Setup}
Machine learning aims to minimize $\mathcal{R}\left(f_{\mathcal{A}(S)}\right)$. However, this is \textbf{non-computable} as $\mathbb{P}_{\mathcal{(X,Y)}}$ is unknown. Therefore, one minimizes the empirical risk
$$\mathcal{R}_{S}\left(f_{\mathcal{A}(S)}\right)=\frac{1}{\vert S\vert}\sum_{(x,y)\in S}\mathcal{L}(f_{\mathcal{A}(S)}(x),y),$$
where the generalization gap is given by $\mathcal{R}\left(f_{\mathcal{A}(S)}\right)-\mathcal{R}_S\left(f_{\mathcal{A}(S)}\right).$
\section{Generalization bounds via validation}
The training-validation paradigm involves holding out a validation set to optimize model architecture. Giving rise to the hypothesis that \textit{deep neural networks can obtain good generalization error by performing a model search on the validation set.}
\begin{proposition}
    Let $S_{m_{\mathrm{val}}}^{\mathrm{(val)}}$ be a held-out validation set, where $\vert S_{m_{\mathrm{val}}}^{\mathrm{(val)}}\vert=m_{\mathrm{(val)}}$. Assume that $m_{\mathrm{(val)}}$ is an $\mathrm{i.i.d}$ sample from $\mathbb{P}_{\mathcal(X,Y)}$. Let $\kappa_{f,i}=\mathcal{R}(f)-\mathcal{L}(f(x_i),y_i)$ for $(x_i,y_i)\in S_{m_{\mathrm{val}}}^{\mathrm{(val)}}$. Suppose that $\mathbb{E}(\kappa_{f,i}^2)\leq\gamma^2$ and $\vert\kappa_{f,i}\vert\leq C$ almost surely for all $(f,i)\in\mathcal{F}_{\mathrm{val}}\times\{1,\dots,m_{\mathrm{val}}\}$. Then, for $\delta\in(0,1]$, with probability $1-\delta$
    $$\mathcal{R}(f)\leq\mathcal{R}_{S_{m_{\mathrm{val}}}^{\mathrm{(val)}}}(f)+\frac{2C\log\left(\frac{\vert\mathcal{F}_{\mathrm{val}}\vert}{\delta}\right)}{3m_{\mathrm{val}}}+\sqrt{\frac{2\gamma^2\log\left(\frac{\vert\mathcal{F}_{\mathrm{val}}\vert}{\delta}\right)}{m_{\mathrm{val}}}}$$
    holds for all $f\in\mathcal{F}_{\mathrm{val}}$.
\end{proposition}
\begin{remarks}
\
\begin{itemize}
    \item $\mathcal{F}_{\mathrm{val}}$ is independent of $S_{m_{\mathrm{val}}}^{\mathrm{(val)}}$.
    \item The bound is only dependent on the validation error on $S_{m_{\mathrm{val}}}^{\mathrm{(val)}}$.
\end{itemize}
\end{remarks}
\begin{implementation}
    
\end{implementation}

\end{document}