# Generalization-of-Neural-Networks

This repository contains the code for an undergraduate research project I conducted in the summer of 2023. The project looks into the current state of theory regarding the generalization of deep neural networks. Throughout the repository, the generalization bounds proposed by various papers are tested and investigated. Furthermore, some of the work is my own work that extends the concepts in the papers and links them together in a concise and coherent manner. The aim of the project is to understand how current literature intertwines to give us an understanding of how deep neural networks generalize.

| Reference      | Paper PDF | Part of Project|Section of Project Report | Project Code
| ----------- | ----------- | ----------- |----------- | ----------- |
| Computing Nonvacuous Generalization Bonds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data| [PDF](https://arxiv.org/pdf/1703.11008.pdf)| |PAC-Bayes | [CODE](https://github.com/ThomasWalker1/Generalization-of-Neural-Networks/tree/main/Generalization%20in%20Deep%20Learning/PAC) |
| Stronger Generalization Bounds for Deep Nets via a Compression Approach |[PDF](https://arxiv.org/pdf/1802.05296.pdf)| |Compression | [CODE](https://github.com/ThomasWalker1/Generalization-of-Neural-Networks/tree/main/Generalization%20in%20Deep%20Learning/Compression) |
| Non-Vacuous Generalization Bounds at the ImageNet Scale: A PAC-Bayesian Compression Approach|[PDF](https://arxiv.org/pdf/1804.05862.pdf)| |PAC Compression Bounds| CODE |
| Opening the Black Box of Deep Neural Networks via Information|[PDF](https://arxiv.org/pdf/1703.00810.pdf)| |Information Approach| [CODE](https://github.com/ThomasWalker1/Generalization-of-Neural-Networks/tree/main/Generalization%20in%20Deep%20Learning/Information) |
| Towards Understanding the Role of Over-Parameterization in Generalization of Neural Networks|[PDF](https://arxiv.org/pdf/1805.12076.pdf)| |Unit-Wise Capacity Measures| [CODE](https://github.com/ThomasWalker1/Generalization-of-Neural-Networks/tree/main/Generalization%20in%20Deep%20Learning/Unit-Wise%20Capacity) |
| Implicit Regularization via Neural Feature Alignment|[PDF](https://arxiv.org/pdf/2008.00938.pdf)| |Neural Tangent Kernel| [CODE](https://github.com/ThomasWalker1/Generalization-of-Neural-Networks/tree/main/Generalization%20in%20Deep%20Learning/Tangent%20Kernel) |
| On the Generalization Mystery in Deep Learning|[PDF](https://arxiv.org/pdf/2203.10036.pdf)| |Algorithmic Stability | [CODE](https://github.com/ThomasWalker1/Generalization-of-Neural-Networks/tree/main/Generalization%20in%20Deep%20Learning/Gradients) |
| Stiffness: A New Perspective on Generalization in Neural Networks|[PDF](https://arxiv.org/pdf/1901.09491.pdf)| |Stiffness | [CODE](https://github.com/ThomasWalker1/Generalization-of-Neural-Networks/tree/main/Generalization%20in%20Deep%20Learning/Stiffness) |
| Why Neural Networks Find Simple Solutions: The Many Regularizers of Geometric Complexity|[PDF](https://arxiv.org/pdf/2209.13083.pdf)| |Geometric Complexity Measure | [CODE](https://github.com/ThomasWalker1/Generalization-of-Neural-Networks/tree/main/Generalization%20in%20Deep%20Learning/Geometric%20Complexity) |
| Generalization In Deep Learning|[PDF](https://arxiv.org/pdf/1710.05468.pdf)| |Validation Paradigm | [CODE](https://github.com/ThomasWalker1/Generalization-of-Neural-Networks/tree/main/Generalization%20in%20Deep%20Learning/Validation) |
