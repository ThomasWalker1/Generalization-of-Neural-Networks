# Problem Outline

Suppose that we have a network that we train to zero empirical error on some training set $S\in\mathcal{Z}^m$, where $\mathcal{Z}$ is data space. We have bounds on the true error of the network, say $B(m)$, that hold with high probability and are a function of the number of samples in our training set. It is reasonable to assume that our bound is decreasing in $m$, otherwise it wouldn't align with our intuition that a larger training set corresponds to a better trained network. We can consider the improvement in our bound when we correctly train to an added data point in the training set, $$\delta_m=B(m)-B(m+1).$$ Instead suppose that we can guarantee zero error on a polytope $\Delta\subset\mathcal{Z}$. The question is to now understand the improvement in our bound with this added information. Suppose that the unknown underlying distribution on our data space $\mathcal{Z}=\mathcal{X}\times\mathcal{Y}$ is given by $\mathcal{D}$. Our loss function is $l:\mathcal{Y}\times\mathcal{Y}\to\mathbb{R}_+$. Let the network $h_{\mathbf{w}}$ be parameterized by the weight vector $\mathbf{w}\in\mathcal{W}$ so that the true error of our network is $$R(\mathbf{w})=\mathbb{E}_{(x,y)\sim\mathcal{D}}\left(l(h_{\mathbf{w}}(x),y)\right).$$ We will work with the more concise notation $l_z(\mathbf{w})=l(h_{\mathbf{w}}(x),y)$ for $z=(x,y)\in\mathcal{Z}$ so that $$R(\mathbf{w})=\mathbb{E}_{z\sim\mathcal{D}}\left(l_z(\mathbf{w})\right).$$ As we obtain zero training error on the polytope $\Delta$ we know what the distribution $\mathcal{D}$ looks like for $z\in\Delta$ and so we can explicitly calculate $$p_{\Delta}=\mathbb{P}_{\mathcal{D}}\left(z\in\Delta\right)=\int_{\Delta}\mathcal{D}(z)dz.$$

# Attempt 1

Instead of working the State of the Art I will first work with the following bound.

**Theorem** Let $\vert\mathcal{W}\vert=M<\infty$, $\delta\in(0,1)$ and $\mathbf{w}\in\mathcal{W}$ then it follows that $$\mathbb{P}_{S\sim\mathcal{D}^m}\left(R(\mathbf{w})\leq\hat{R}(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)\geq1-\delta.$$

Lets adopt the notation $[k]={1,\dots,k}$ and $[[k]]_m={k,\dots,m}$ for $k,m\in\mathbb{N}$. Suppose that we achieve zero training error on the polytope $\Delta$. Observe that, $$\begin{align*}\mathbb{P}_{S\sim\mathcal{D}^m}\left(R(\mathbf{w})>\hat{R}(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)&=\sum_{k=0}^m\mathbb{P}_{S\sim\mathcal{D}^m}\left(R(\mathbf{w})>\hat{R}(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\Bigg\vert z_{[k]}\notin\Delta,z_{[[k+1]]_m}\in\Delta\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k.\end{align*}$$ For each $k$ as $l_{z_i}=0$ for $i\in[[k+1]]_m$ we can let empirical loss of the first $k$ terms, $\hat{R}_k(\mathbf{w})$, be $\frac{m}{k}$ times larger and then consider the bounds holding for sample sizes of $k$. Therefore, $$\begin{align*}\mathbb{P}_{S\sim\mathcal{D}^m}\left(R(\mathbf{w})>\hat{R}(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)&=\delta\mathbb{I}\left(R(\mathbf{w})>C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)p_{\Delta}^m+\sum_{k=1}^m\mathbb{P}_{S\sim\mathcal{D}^k}\left(R(\mathbf{w})>\frac{m}{k}\hat{R}_k(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k\\&=\delta\mathbb{I}\left(R(\mathbf{w})>C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)p_{\Delta}^m+\sum_{k=1}^m\mathbb{P}_{S\sim\mathcal{D}^k}\left(R(\mathbf{w})>\hat{R}_k(\mathbf{w})+\frac{m-k}{k}\hat{R}_k(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k.\end{align*}$$ Now let $\tilde{\delta}_i$ be such that $$\frac{m-k}{k}\hat{R}_k(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}=C\sqrt{\frac{\log\left(\frac{M}{\delta_i}\right)}{2k}}$$ and let $\delta_i=\min\left(\delta,\tilde{\delta}_i\right)$ so that $$\begin{align*}\mathbb{P}_{S\sim\mathcal{D}^m}\left(R(\mathbf{w})>\hat{R}(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)&=\delta\mathbb{I}\left(R(\mathbf{w})>C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)p_{\Delta}^m+\sum_{k=1}^m\mathbb{P}_{S\sim\mathcal{D}^k}\left(R(\mathbf{w})>\hat{R}_k(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta_i}\right)}{2m}}\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k\\&\leq \delta p_{\Delta}^m+\sum_{k=1}^m\delta_ip_{\Delta}^{m-k}(1-p_{\Delta})^k\\&\leq\delta.\end{align*}$$ Therefore, with the added information that the train error is zero on $\delta$ we can deduce that our bound occurs with higher probability.

# Attempt 2

Suppose that we have our dataset and the knowledge that we achieve zero training error on the polytope $\Delta$. Then following the same steps of Theorem 2.1 with the added information we can derive a stronger bound. As in the proof we use Markov's inequality, so note that $$\begin{align*}\mathbb{E}_{S\sim\mathcal{D}^m}\left(\exp\left(t\sum_{i=1}^m(\mathbb{E}(l_{z_i}(\mathbf{w}))-l_{z_i}(\mathbf{w}))\right)\right)&=\sum_{k=0}^m\mathbb{E}_{S\sim\mathcal{D}^m}\left(\exp\left(t\sum_{i=1}^m(\mathbb{E}(l_{z_i}(\mathbf{w}))-l_{z_i}(\mathbf{w}))\right)\Bigg\vert z_{[i]}\not\in\Delta,z_{[[k]]}\in\Delta\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k\\&\leq\sum_{k=0}^m\exp\left(\frac{kt^2C^2}{8}\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k.\end{align*}$$ Therefore, we get by Markov's inequality that $$\mathbb{P}_{S\sim\mathcal{D}^m}\left(R(\mathbf{w})>\hat{R}(\mathbf{w})+s\right)\leq\frac{1}{\exp(mts)}\sum_{k=0}^m\exp\left(\frac{kt^2C^2}{8}\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k.$$ One can find the value $t^*$ for $t$ for which this bound is minimized. Then we apply a uniform bound argument to get that $$\mathbb{P}_{S\sim\mathcal{D}^m}\left(\sup_{\mathbf{w}\in\mathcal{W}}\left(R(\mathbf{w})-\hat{R}(\mathbf{w})\right)>s\right)\leq M\frac{\sum_{k=0}^m\exp\left(\frac{kt^2C^2}{8}\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k}{\exp(mts)}.$$ Taking $\delta$ to be the right-hand side of this expression and considering the complement we get a tighter bound for $R(\mathbf{w})$ that holds with the same high probability.

# Directions to Proceed

1. As the polytopes contain an potentially infinite number of points, if we apply the bound naively we get a bound of zero.
    - Need to come up with continuous versions of the bounds rather than discrete bounds
2. Understand what properties of the polytopes would lead to tighter bounds.
3. How can we exploit the fact that we know a portion of the underlying distribution?