# Problem Outline

Suppose that we have a network that we train on a training set $S\in\mathcal{Z}^m$, where $\mathcal{Z}=\mathcal{X}\times\mathcal{Y}$ is the data space. We have bounds on the true error of the network, say $B(m)$, that hold with high probability and are a function of the number of samples in our training set. It is reasonable to assume that our bound is decreasing in $m$, otherwise it wouldn't align with our intuition that a larger training set corresponds to a better trained network. We can consider the improvement in our bound when we correctly train to an additional data point, $$\delta B_m=B(m)-B(m+1).$$ Instead suppose that we can guarantee zero error on a polytope $\Delta\subset\mathcal{Z}$. The question is to now understand the improvement in our bound with this added information. Suppose that the unknown underlying distribution on our data space $\mathcal{Z}$ is given by $\mathcal{D}$. Let the loss function be $l:\mathcal{Y}\times\mathcal{Y}\to\mathbb{R}_+$. Let the network $h_{\mathbf{w}}$ be parameterized by the weight vector $\mathbf{w}\in\mathcal{W}$ so that the true error of our network can be written as $$R(\mathbf{w})=\mathbb{E}_{(x,y)\sim\mathcal{D}}\left(l(h_{\mathbf{w}}(x),y)\right).$$ We will work with the more concise notation $l_z(\mathbf{w})=l(h_{\mathbf{w}}(x),y)$ for $z=(x,y)\in\mathcal{Z}$ so that $$R(\mathbf{w})=\mathbb{E}_{z\sim\mathcal{D}}\left(l_z(\mathbf{w})\right).$$ As we obtain zero training error on the polytope $\Delta$ we know what the distribution $\mathcal{D}$ looks like for $z\in\Delta$ and so we can explicitly calculate $$p_{\Delta}=\mathbb{P}_{\mathcal{D}}\left(z\in\Delta\right)=\int_{\Delta}\mathcal{D}(z)dz.$$ There are two ways that this information could help us, it could either helps us to obtain tighter bounds on the true error, or it could improve the confidence that we have in our bound. Instead of working the State of the Art I will first work with the following bound.

**Theorem** Let $\vert\mathcal{W}\vert=M<\infty$, $\delta\in(0,1)$ and $\mathbf{w}\in\mathcal{W}$ then it follows that $$\mathbb{P}_{S\sim\mathcal{D}^m}\left(R(\mathbf{w})\leq\hat{R}(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)\geq1-\delta.$$

# Improving the Confidence on the Bound

Let us adopt the notation $[k]={1,\dots,k}$ and $[[k]]_m={k,\dots,m}$ for $k,m\in\mathbb{N}$. To capitalize on the fact that we know we have zero training error on the polytope $\Delta$ we consider, $$\begin{align*}\mathbb{P}_{S\sim\mathcal{D}^m}\left(R(\mathbf{w})>\hat{R}(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)&=\sum_{k=0}^m\mathbb{P}_{S\sim\mathcal{D}^m}\left(R(\mathbf{w})>\hat{R}(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\Bigg\vert z_{[k]}\notin\Delta,z_{[[k+1]]_m}\in\Delta\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k.\end{align*}$$ For each $k$ as $l_{z_i}=0$ for $i\in[[k+1]]_m$ we can let the empirical error of the first $k$ terms, $\hat{R}_k(\mathbf{w})$, be $\frac{m}{k}$ times larger and then consider the bounds holding for samples of size $k$. Therefore, $$\begin{align*}\mathbb{P}_{S\sim\mathcal{D}^m}\left(R(\mathbf{w})>\hat{R}(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)&=\delta\mathbb{I}\left(R(\mathbf{w})>C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)p_{\Delta}^m+\sum_{k=1}^m\mathbb{P}_{S\sim\mathcal{D}^k}\left(R(\mathbf{w})>\frac{m}{k}\hat{R}_k(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k\\&=\delta\mathbb{I}\left(R(\mathbf{w})>C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)p_{\Delta}^m+\sum_{k=1}^m\mathbb{P}_{S\sim\mathcal{D}^k}\left(R(\mathbf{w})>\hat{R}_k(\mathbf{w})+\frac{m-k}{k}\hat{R}_k(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k.\end{align*}$$ Now let $\tilde{\delta}_i$ be such that $$\frac{m-k}{k}\hat{R}_k(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}=C\sqrt{\frac{\log\left(\frac{M}{\delta_i}\right)}{2k}}$$ and let $\delta_i=\min\left(\delta,\tilde{\delta}_i\right)$ so that $$\begin{align*}\mathbb{P}_{S\sim\mathcal{D}^m}\left(R(\mathbf{w})>\hat{R}(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)&=\delta\mathbb{I}\left(R(\mathbf{w})>C\sqrt{\frac{\log\left(\frac{M}{\delta}\right)}{2m}}\right)p_{\Delta}^m+\sum_{k=1}^m\mathbb{P}_{S\sim\mathcal{D}^k}\left(R(\mathbf{w})>\hat{R}_k(\mathbf{w})+C\sqrt{\frac{\log\left(\frac{M}{\delta_i}\right)}{2m}}\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k\\&\leq \delta p_{\Delta}^m+\sum_{k=1}^m\delta_ip_{\Delta}^{m-k}(1-p_{\Delta})^k\\&\leq\delta.\end{align*}$$ Therefore, with the added information we can potentially deduce that our bound occurs with higher probability.

# Improving the Bound

Following the same steps of Theorem 2.1 with the added information we can derive a stronger bound. As in the proof we apply Markov's inequality so first note that $$\begin{align*}\mathbb{E}_{S\sim\mathcal{D}^m}\left(\exp\left(t\sum_{i=1}^m(\mathbb{E}(l_{z_i}(\mathbf{w}))-l_{z_i}(\mathbf{w}))\right)\right)&=\sum_{k=0}^m\mathbb{E}_{S\sim\mathcal{D}^m}\left(\exp\left(t\sum_{i=1}^m(\mathbb{E}(l_{z_i}(\mathbf{w}))-l_{z_i}(\mathbf{w}))\right)\Bigg\vert z_{[i]}\not\in\Delta,z_{[[k]]}\in\Delta\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k\\&\leq\sum_{k=0}^m\exp\left(\frac{kt^2C^2}{8}\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k.\end{align*}$$ Therefore, we get by Markov's inequality that $$\mathbb{P}_{S\sim\mathcal{D}^m}\left(R(\mathbf{w})>\hat{R}(\mathbf{w})+s\right)\leq\frac{1}{\exp(mts)}\sum_{k=0}^m\exp\left(\frac{kt^2C^2}{8}\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k.$$ One can find the value $t^*$ for $t$ for which this bound is minimized. Then we apply a union bound argument to get that $$\mathbb{P}_{S\sim\mathcal{D}^m}\left(\sup_{\mathbf{w}\in\mathcal{W}}\left(R(\mathbf{w})-\hat{R}(\mathbf{w})\right)>s\right)\leq M\frac{\sum_{k=0}^m\exp\left(\frac{kt^2C^2}{8}\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k}{\exp(mts)}.$$ Taking $\delta$ to be the right-hand side of this expression and considering the complement we get a tighter bound for $R(\mathbf{w})$ that holds with the same high probability. Note that this is tighter than the original bound as $$\begin{align*}\sum_{k=0}^m\exp\left(\frac{kt^2C^2}{8}\right)p_{\Delta}^{m-k}(1-p_{\Delta})^k&\leq\exp\left(\frac{nt^2C^2}{8}\right)\sum_{k=0}^mp_{\Delta}^{m-k}(1-p_{\Delta})^k\\&=\exp\left(\frac{nt^2C^2}{8}\right).\end{align*}$$

# Directions to Proceed

1. As the polytopes contain an potentially infinite number of points, if we apply the bound naively we get a bound of zero.
    - Need to come up with continuous versions of the bounds rather than discrete bounds
2. Understand what properties of the polytopes would lead to tighter bounds.
3. How can we exploit the fact that we know a portion of the underlying distribution?